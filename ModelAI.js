const fs = require("fs"); // Module de gestion des fichiers en Node.js

// Configuration de la grille
const gridSize = 1000;
const historySize = 10; // Profondeur de l'historique pour la pr√©diction des mouvements

// Param√®tres d'apprentissage par renforcement (Q-Learning)
let QTable = {}; // Q-Table : associe un √©tat aux meilleures actions possibles
const epsilon = 0.05; // Taux d'exploration pour la s√©lection des actions
const alpha = 0.1; // Taux d'apprentissage pour la mise √† jour des valeurs
const gamma = 0.9; // Facteur d'escompte pour les r√©compenses futures
const saveInterval = 10; // Fr√©quence d'enregistrement de la Q-Table

// Initialisation du serpent et de la b√™te
let pastSnakePositions = [];
let beast = { x: 50, y: 50 };
let snake = { x: 300, y: 300 };

// D√©placements possibles : 4 directions cardinales + 4 diagonales
const actions = [
    { dx: 10, dy: 0 }, { dx: -10, dy: 0 }, { dx: 0, dy: 10 }, { dx: 0, dy: -10 },
    { dx: 10, dy: 10 }, { dx: -10, dy: 10 }, { dx: 10, dy: -10 }, { dx: -10, dy: -10 }
];

/**
 * Charge la Q-Table √† partir d'un fichier JSON
 * @param {string} filename - Chemin du fichier de sauvegarde
 */
function loadQTableFromFile(filename = "q_table.json") {
    if (!fs.existsSync(filename)) {
        console.warn(`‚ö†Ô∏è Aucun fichier Q-Table trouv√©, d√©marrage √† neuf.`);
        return;
    }
    try {
        QTable = JSON.parse(fs.readFileSync(filename, "utf-8"));
        console.log(`üìÇ Q-Table charg√©e avec ${Object.keys(QTable).length} entr√©es.`);
    } catch (error) {
        console.error(" Erreur lors du chargement de la Q-Table :", error);
    }
}

/**
 * Sauvegarde la Q-Table dans un fichier JSON
 * @param {string} filename - Chemin du fichier de sauvegarde
 */
function saveQTableToFile(filename = "q_table.json") {
    fs.writeFileSync(filename, JSON.stringify(QTable, null, 2), "utf-8");
    console.log(` Q-Table mise √† jour et enregistr√©e.`);
}

/**
 * Pr√©dit la future position du serpent en utilisant une r√©gression pond√©r√©e
 * @returns {Object} - Coordonn√©es pr√©dites (x, y) du serpent
 */
function predictSnakeFuturePosition() {
    if (pastSnakePositions.length < 2) {
        return pastSnakePositions.length > 0 ? pastSnakePositions[pastSnakePositions.length - 1] : { ...snake };
    }

    let sumWX = 0, sumWY = 0, sumWT = 0, sumWXY = 0, sumWX2 = 0, sumW = 0;
    let n = Math.min(pastSnakePositions.length, historySize);
    let recentPositions = pastSnakePositions.slice(-n);

    for (let i = 0; i < n; i++) {
        let t = i + 1;
        let weight = Math.exp(i - n);
        sumWX += weight * t;
        sumWY += weight * recentPositions[i].x;
        sumWXY += weight * t * recentPositions[i].x;
        sumWX2 += weight * t * t;
        sumW += weight;
    }

    let slopeX = (sumW * sumWXY - sumWX * sumWY) / (sumW * sumWX2 - sumWX * sumWX);
    let interceptX = (sumWY - slopeX * sumWX) / sumW;
    
    sumWY = sumWXY = sumWX2 = 0;
    for (let i = 0; i < n; i++) {
        let t = i + 1;
        let weight = Math.exp(i - n);
        sumWY += weight * recentPositions[i].y;
        sumWXY += weight * t * recentPositions[i].y;
        sumWX2 += weight * t * t;
    }

    let slopeY = (sumW * sumWXY - sumWX * sumWY) / (sumW * sumWX2 - sumWX * sumWX);
    let interceptY = (sumWY - slopeY * sumWX) / sumW;
    
    let predictedX = Math.round((slopeX * (n + 3) + interceptX) / 10) * 10;
    let predictedY = Math.round((slopeY * (n + 3) + interceptY) / 10) * 10;
    
    return { x: predictedX, y: predictedY };
}

/**
 * R√©cup√®re l'√©tat actuel de l'environnement sous forme de cha√Æne de caract√®res
 * @returns {string} - Repr√©sentation de l'√©tat actuel
 */
function getCurrentState() {
    return `${beast.x},${beast.y},${snake.x},${snake.y}`;
}

/**
 * S√©lectionne la meilleure action pour la b√™te en utilisant Q-Learning
 * @param {string} state - √âtat actuel
 * @returns {Object} - Action choisie (dx, dy)
 */
function chooseBestAction(state) {
    if (!QTable[state] || Math.random() < epsilon) {
        return actions[Math.floor(Math.random() * actions.length)]; // Exploration
    }

    return actions.reduce((best, action) => {
        let key = `${state}-${action.dx}-${action.dy}`;
        return (!QTable[key] || QTable[key] > QTable[`${state}-${best.dx}-${best.dy}`]) ? action : best;
    });
}

/**
 * Met √† jour la Q-Table en fonction de la r√©compense re√ßue
 * @param {string} state - √âtat pr√©c√©dent
 * @param {Object} action - Action effectu√©e (dx, dy)
 * @param {number} reward - R√©compense obtenue
 * @param {string} newState - Nouvel √©tat apr√®s l'action
 */
function updateQTable(state, action, reward, newState) {
    let key = `${state}-${action.dx}-${action.dy}`;
    if (!QTable[key]) QTable[key] = Math.random() * 0.01;

    let futureQ = Math.max(...actions.map(a => QTable[`${newState}-${a.dx}-${a.dy}`] || 0));
    QTable[key] = (1 - alpha) * QTable[key] + alpha * (reward + gamma * futureQ);

    saveQTableToFile(); // Enregistrer imm√©diatement apr√®s mise √† jour
}

/**
 * Met √† jour la position de la b√™te en suivant la politique d'apprentissage
 */
function updateBeast() {
    let state = getCurrentState();
    let action = chooseBestAction(state);
    let newBeast = { x: beast.x + action.dx, y: beast.y + action.dy };

    // Assurer que le d√©placement reste dans les limites de la grille
    newBeast.x = Math.max(0, Math.min(newBeast.x, gridSize - 1));
    newBeast.y = Math.max(0, Math.min(newBeast.y, gridSize - 1));

    let predictedSnake = predictSnakeFuturePosition();
    let reward = 0; // Placeholder pour le calcul de la r√©compense

    let newState = `${newBeast.x},${newBeast.y},${snake.x},${snake.y}`;
    updateQTable(state, action, reward, newState);
    beast = newBeast; // Appliquer le d√©placement

    console.log(`üêç B√™te d√©plac√©e √† : (${beast.x}, ${beast.y})`);
}

updateBeast();